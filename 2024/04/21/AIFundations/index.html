<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.0.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":"flat","style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"utterances","storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Abstract. AI 基础复习笔记。背书。 里面有一些不知道有什么道理的批话，均用斜体表出。    历史 Math for AI 最大似然估计（MLE） 最大后验概率（MAP） 信息论   机器学习选讲 线性回归 逻辑回归 其他东西   神经网络 CNN 批话 代表性网络 应用 Object Detection Image Segmentation Face Recognition     G">
<meta property="og:type" content="article">
<meta property="og:title" content="Revision | AI 基础（期中）">
<meta property="og:url" content="http://example.com/2024/04/21/AIFundations/index.html">
<meta property="og:site_name" content="Stellary&#39;s Notes">
<meta property="og:description" content="Abstract. AI 基础复习笔记。背书。 里面有一些不知道有什么道理的批话，均用斜体表出。    历史 Math for AI 最大似然估计（MLE） 最大后验概率（MAP） 信息论   机器学习选讲 线性回归 逻辑回归 其他东西   神经网络 CNN 批话 代表性网络 应用 Object Detection Image Segmentation Face Recognition     G">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/picture/ReLUfitCurve.png">
<meta property="og:image" content="http://example.com/picture/SpatialPyramid.png">
<meta property="og:image" content="http://example.com/picture/MobileNet.png">
<meta property="og:image" content="http://example.com/picture/FCN.png">
<meta property="og:image" content="http://example.com/picture/SkipConnection.jpg">
<meta property="og:image" content="http://example.com/picture/SegNet.png">
<meta property="og:image" content="http://example.com/picture/PSPNet.png">
<meta property="og:image" content="http://example.com/picture/GAN.png">
<meta property="og:image" content="http://example.com/picture/conGAN.png">
<meta property="og:image" content="http://example.com/picture/BruteLatent.jpg">
<meta property="og:image" content="http://example.com/picture/BiGAN.jpg">
<meta property="og:image" content="http://example.com/picture/CoGAN.png">
<meta property="og:image" content="http://example.com/picture/CycleGAN.jpg">
<meta property="og:image" content="http://example.com/picture/VanillaRNN.png">
<meta property="og:image" content="http://example.com/picture/LSTM.png">
<meta property="og:image" content="http://example.com/picture/Transformer.png">
<meta property="og:image" content="http://example.com/picture/GPTRefine.jpg">
<meta property="article:published_time" content="2024-04-21T06:15:59.000Z">
<meta property="article:modified_time" content="2024-04-22T04:03:54.055Z">
<meta property="article:author" content="King Strange">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/picture/ReLUfitCurve.png">


<link rel="canonical" href="http://example.com/2024/04/21/AIFundations/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2024/04/21/AIFundations/","path":"2024/04/21/AIFundations/","title":"Revision | AI 基础（期中）"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Revision | AI 基础（期中） | Stellary's Notes</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Stellary's Notes</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">历史</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">2.</span> <span class="nav-text">Math for AI</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">2.1.</span> <span class="nav-text">最大似然估计（MLE）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">2.2.</span> <span class="nav-text">最大后验概率（MAP）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">2.3.</span> <span class="nav-text">信息论</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">3.</span> <span class="nav-text">机器学习选讲</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">3.1.</span> <span class="nav-text">线性回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">3.2.</span> <span class="nav-text">逻辑回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">3.3.</span> <span class="nav-text">其他东西</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">4.</span> <span class="nav-text">神经网络</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">5.</span> <span class="nav-text">CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">5.1.</span> <span class="nav-text">批话</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">5.2.</span> <span class="nav-text">代表性网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">5.3.</span> <span class="nav-text">应用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">5.3.1.</span> <span class="nav-text">Object Detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">5.3.2.</span> <span class="nav-text">Image Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">5.3.3.</span> <span class="nav-text">Face Recognition</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">6.</span> <span class="nav-text">GAN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">6.1.</span> <span class="nav-text">经典算法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">7.</span> <span class="nav-text">RNN</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">8.</span> <span class="nav-text">Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">8.1.</span> <span class="nav-text">网络结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">8.2.</span> <span class="nav-text">预训练模型</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="King Strange"
      src="https://s2.loli.net/2023/11/24/rtCEFG1igyYAlm4.png">
  <p class="site-author-name" itemprop="name">King Strange</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">20</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/SocialZxy" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;SocialZxy" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:stellary@stu.pku.edu.cn" title="E-Mail → mailto:stellary@stu.pku.edu.cn" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/04/21/AIFundations/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://s2.loli.net/2023/11/24/rtCEFG1igyYAlm4.png">
      <meta itemprop="name" content="King Strange">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Stellary's Notes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Revision | AI 基础（期中） | Stellary's Notes">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Revision | AI 基础（期中）
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-04-21 14:15:59" itemprop="dateCreated datePublished" datetime="2024-04-21T14:15:59+08:00">2024-04-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-04-22 12:03:54" itemprop="dateModified" datetime="2024-04-22T12:03:54+08:00">2024-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Revision-Notes/" itemprop="url" rel="index"><span itemprop="name">Revision Notes</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><strong>Abstract.</strong> AI 基础复习笔记。背书。</p>
<p>里面有一些不知道有什么道理的批话，均用斜体表出。</p>
<!-- toc -->

<ul>
<li><a href="#%E5%8E%86%E5%8F%B2">历史</a></li>
<li><a href="#math-for-ai">Math for AI</a><ul>
<li><a href="#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1mle">最大似然估计（MLE）</a></li>
<li><a href="#%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87map">最大后验概率（MAP）</a></li>
<li><a href="#%E4%BF%A1%E6%81%AF%E8%AE%BA">信息论</a></li>
</ul>
</li>
<li><a href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%89%E8%AE%B2">机器学习选讲</a><ul>
<li><a href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92">线性回归</a></li>
<li><a href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92">逻辑回归</a></li>
<li><a href="#%E5%85%B6%E4%BB%96%E4%B8%9C%E8%A5%BF">其他东西</a></li>
</ul>
</li>
<li><a href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">神经网络</a></li>
<li><a href="#cnn">CNN</a><ul>
<li><a href="#%E6%89%B9%E8%AF%9D">批话</a></li>
<li><a href="#%E4%BB%A3%E8%A1%A8%E6%80%A7%E7%BD%91%E7%BB%9C">代表性网络</a></li>
<li><a href="#%E5%BA%94%E7%94%A8">应用</a><ul>
<li><a href="#object-detection">Object Detection</a></li>
<li><a href="#image-segmentation">Image Segmentation</a></li>
<li><a href="#face-recognition">Face Recognition</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#gan">GAN</a><ul>
<li><a href="#%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95">经典算法</a></li>
</ul>
</li>
<li><a href="#rnn">RNN</a></li>
<li><a href="#transformer">Transformer</a><ul>
<li><a href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84">网络结构</a></li>
<li><a href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B">预训练模型</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<span id="more"></span>

<h1><span id="历史">历史</span></h1><p>1955, Dartmouth 会议提出人工智能一词。</p>
<ul>
<li><strong>初心：有智能的机器。</strong> [Turing, 1948]<ul>
<li>任务：定理证明，下棋，说话，看图，像人一样行动，</li>
<li>目标：通用。</li>
<li>路径：将人类智慧输入机器（符号派）、神经网络（连接派），强化学习（行为派）。</li>
</ul>
</li>
<li><strong>早期：百花齐放</strong><ul>
<li><strong>Logical Theorist</strong>，1955-1956，数学定理（数学原理）证明，启发式搜索。</li>
<li><strong>IBM 深蓝</strong>，1985，国际象棋，Minimax 搜索、Alpha-Beta 剪枝。</li>
<li><strong>Georgetown 实验</strong>，1953-1954，机器翻译（有机化学领域）。</li>
<li><strong>ELIZA</strong>，1964-1966，对话，关键词打分、查找表。</li>
<li><strong>SHRDLU（积木世界）</strong>，1968-1970，回答并按要求挪动物品（拥有记忆单元），逻辑推理。</li>
<li><strong>STRIPS</strong>，1971，自动规划问题。</li>
<li><strong>Conway Game</strong>，1950，模拟生物细胞自我复制。</li>
<li><strong>Perceptron</strong>，1943，图像识别，单层神经网络。不能解决异或问题。</li>
</ul>
</li>
<li><strong>中期：符号主义</strong><ul>
<li><strong>自动定理证明</strong>：王浩 1958（一阶逻辑），吴文俊（几何）。</li>
<li><strong>Dendral</strong>（第一个专家系统软件）：有机化学，推理分子组成和结构。</li>
<li><strong>MYCIN</strong>：识别细菌，推荐抗生素。推理引擎、知识库、不精确推理。</li>
<li><strong>知识库</strong>：Cyc，Semantic Network，WordNet $\Rightarrow$ IBM watson。</li>
<li><strong>日本五代机</strong>：失败。</li>
</ul>
</li>
<li><strong>近期：联结主义</strong><ul>
<li><strong>AlexNet</strong>：CNN（ReLU，Dropout，LRN）</li>
<li><strong>AlphaGo</strong>：监督学习，强化学习，MCTS。AlphaGo Zero：放弃人类专家经验，从零开始学习。</li>
<li><strong>ChatGPT</strong>。</li>
</ul>
</li>
</ul>
<h1><span id="math-for-ai">Math for AI</span></h1><div class="note info"><p><strong>Theorem.(Bayes)</strong></p>
<p>$$<br>p(Y|X) &#x3D; \frac{p(X|Y)p(Y)}{\sum_Y p(X|Y)p(Y)} \propto p(X|Y)p(Y)<br>$$</p>
</div>

<div class="note success"><p><strong>Nomenclatures.</strong></p>
<ul>
<li>后验概率（posterior）：$P(Y|X)$，观察到结果 $X$ 之后的概率。</li>
<li>似然（likelihood）：$P(X|Y)$，对于参数设置，观察到数据集的可能性。</li>
<li>先验（prior）：$P(Y)$，对参数分布的猜测。</li>
</ul>
<p>分母称为归一化因子。</p>
</div>

<h2><span id="最大似然估计mle">最大似然估计（MLE）</span></h2><p>假设数据独立同分布，给定一个观测到的数据，求一个参数 $\theta$ 来最大化概率，i.e.</p>
<p>$$<br>\mathrm{argmax}_\theta \sum_{i&#x3D;1}^n \log  p(x_i | \theta)<br>$$</p>
<div class="note warning"><p><strong>Example.</strong> 给定一组数据，假设服从正态分布，用 MLE 估计分布。</p>
<details class="note "><summary><p>Hint.</p>
</summary>
<p>直接求偏导即可。可以解出 $\mu$ 就是本组数据的均值，$\sigma^2$ 就是本组数据的方差。</p>

</details></div>

<h2><span id="最大后验概率map">最大后验概率（MAP）</span></h2><p>假设数据独立同分布，给定一个观测到的数据和你对参数 $\theta$ 服从的分布的假设 $p(\theta)$，i.e.</p>
<p>$$<br>\mathrm{argmax}_\theta \sum_{i&#x3D;1}^n \log p(x_i | \theta) p(\theta)<br>$$</p>
<h2><span id="信息论">信息论</span></h2><div class="note success"><p><strong>Definition.(KL-divergence)</strong> </p>
<p>$$<br>D_{KL}(P || Q) &#x3D; \mathbb{E}_{x\sim P} (\log P(x) - \log Q(x))<br>$$</p>
</div>

<p>衡量两个分布之间的差异，根据 Gibss 不等式其总是大于零。感性理解起来这个量表示如果用分布 $Q$ 来解析信息，期望丢失多少信息。</p>
<div class="note success"><p><strong>Definition.(Cross Entropy)</strong></p>
<p>$$<br>H(P, Q) &#x3D; H(P) + D_{KL}(P || Q)<br>$$</p>
</div>

<p>一般最小化交叉熵和最小化 KL 散度等价。</p>
<h1><span id="机器学习选讲">机器学习选讲</span></h1><div class="note warning"><p><strong>大纲.</strong> 机器学习指基于<strong>经验</strong>的算法的设计与分析使得模型在某些<strong>任务</strong>上的<strong>表现</strong>得到提升。</p>
<ul>
<li><strong>任务</strong> 监督学习（分类，回归），无监督学习（聚类，密度估计，降维），半监督学习，弱监督学习，强化学习。</li>
<li><strong>经验</strong> 指训练数据。好的机器学习模型应当不过拟合到训练数据，可以泛化到测试数据。</li>
<li><strong>表现</strong> 有多种方法进行衡量，数学上反映为最小化某个损失函数（0&#x2F;1 loss, square loss, cross entropy loss）等。</li>
</ul>
</div>

<h2><span id="线性回归">线性回归</span></h2><p>给定一组数据 ${(\boldsymbol{x}_1, y_1), …, \boldsymbol{x}_n, y_n}$，用一个模型来估计最接近 $y_i$ 的连续标量。</p>
<p>考虑用一个线性函数来拟合。</p>
<p>$$<br>f(x_i) &#x3D; \boldsymbol{w}^T \boldsymbol{x}_i + b<br>$$</p>
<p>这里可以通过在所有 $\boldsymbol{x}_i$ 后面加一个 $1$ 然后把 $b$ 整合进 $\boldsymbol{w}$ 中来消掉常数项。</p>
<p>采用 $L_2$ Loss 来衡量差异：</p>
<p>$$<br>\mathbb{E}((f(X) - Y)^2)<br>$$</p>
<p>将 $(\boldsymbol{x}_1, …, \boldsymbol{x}_n)$ 记为 $\mathbf{A}$，$(y_1, …, y_n)^T$ 记为 $\boldsymbol{y}$，那么根据线性代数经典结论，$\boldsymbol{w}$ 是</p>
<p>$$<br>\mathbf{A}^T\mathbf{A}\boldsymbol{x} &#x3D; \mathbf{A^T}\boldsymbol{y}<br>$$</p>
<p>的解。该方程总是有解，且在 $\mathbf{A}^T\mathbf{A}$ 可逆时有 closed-form 解。</p>
<p>虽然如此我们不会用矩阵求逆来计算解，而是使用 Gradient Descent 法来求近似解。换言之从一个初始点开始，不断迭代：</p>
<p>$$<br>\boldsymbol{x}’ &#x3D; \boldsymbol{x} + \alpha \nabla L(\boldsymbol{x})<br>$$</p>
<p>其中 $\alpha$ 为学习率，$L$ 为损失函数。如果 $\alpha$ 小，收敛慢，残差小；$\alpha$ 大，收敛快，残差大且可能振动。</p>
<p>如果数据量小于特征维度时 $\mathbf{A}^T\mathbf{A}$ 不可逆，没有 closed-form 解。此时要保证模型不会过拟合到这些数据，依然具有充分好的泛化性能，我们应当对模型进行正则化。</p>
<div class="note warning"><p><strong>Hint.</strong> 正则化为什么是有道理的（正则化的本质）</p>
<p>这里本质上是数据不能给我们足量的信息，于是我们假设参数满足某个先验分布然后做 MAP。即（下面要学习的参数是 $\boldsymbol{\beta}$）</p>
<p>$$<br>\mathrm{argmax}_\theta \log p((\boldsymbol{x}_i, y_i) | \boldsymbol{\beta}, \theta^2) + \log p(\boldsymbol{\beta})<br>$$</p>
<p>假设 $\boldsymbol{\beta}$ 服从</p>
<p>$$<br>p(\boldsymbol{\beta}) \propto \exp(-\boldsymbol{\beta}^\boldsymbol{\beta} &#x2F; \tau^2)<br>$$</p>
<p>此时先验分布一项变成了</p>
<p>$$<br>\lambda ||\boldsymbol{\beta}||^2<br>$$</p>
<p>即 $L_2$ loss 的某个倍数。</p>
<p>如果假设 $\boldsymbol{\beta}$ 服从 Laplace 分布即</p>
<p>$$<br>p(\boldsymbol{\beta}) &#x3D; \exp(|\boldsymbol{\beta}| &#x2F; \tau^2)<br>$$</p>
<p>那么就会产生</p>
<p>$$<br>\lambda ||\boldsymbol{\beta}||<br>$$</p>
<p>分别对应了 Ridge Regression 和 Lasso Regression 正则项。</p>
</div>

<p>从几何角度感性理解，Ridge Regression 生成每一维都较小的解，Lasso Regression 生成稀疏解。</p>
<h2><span id="逻辑回归">逻辑回归</span></h2><p>模型输出</p>
<p>$$<br>\hat{y} &#x3D; f_{w, b}(\boldsymbol{x}) &#x3D; \frac{1}{1 + \exp(-\boldsymbol{w}^t\boldsymbol{x} + b)}<br>$$</p>
<p>本质上是</p>
<p>$$<br>\ln \frac {\hat{y}} {1-\hat{y}} &#x3D; \boldsymbol{w}^T\boldsymbol{x} + b<br>$$</p>
<p>相当于一个线性模型输出分类为正类的相对概率。<em>这时如果 ground truth 为正类，$\hat y$ 就是给定数据，模型输出正类的概率，可以认为是得到某个参数的后验概率，ground truth 为负类时 $1 - \hat{y}$ 的意义同理</em>。</p>
<p>此时假设正类为 $1$ 负类为 $0$，得到 MAP 的目标函数为</p>
<p>$$<br>y \ln f_{w, b}(\boldsymbol{x}) + (1-y) \ln (1 - f_{w, b}(\boldsymbol{x}))<br>$$</p>
<p>发现是真实分布和估计分布之间的交叉熵。</p>
<p>在优化的时候需要计算梯度。简单计算一下可以知道</p>
<p>$$<br>\frac{\partial L}{\partial w_i} &#x3D; \sum_{j&#x3D;1}^n -(y_j - f_{w, b}(\boldsymbol{x}^j))x^j_{i}<br>$$</p>
<p>不采用 $L_2$ Loss 而是采用 Cross Entropy Loss 的理由是 $L_2$ Loss 计算梯度可以得到</p>
<p>$$<br>\frac{\partial L_2}{\partial w_i} &#x3D; 2(f_{w, b}(\boldsymbol{x}) - y)f_{w, b}(\boldsymbol{x})(1-f_{w, b}(\boldsymbol{x}))x_i<br>$$</p>
<p>当 $y &#x3D; 0$ 时，预报接近目标和远离目标均发生梯度消失。</p>
<h2><span id="其他东西">其他东西</span></h2><ul>
<li><strong>Multinomial Logestic Regression</strong> 用 Softmax 函数激活，多分类。</li>
<li><strong>NN Classifier</strong> 和 <strong>$k-$NN Classifier</strong>，超参数为 $k$ 和距离函数。</li>
</ul>
<blockquote>
<p>超参数的确定方法为进行 Cross Validation，在验证集上调参，选取平均结果最好的一组。</p>
</blockquote>
<ul>
<li>$k-$means 聚类的一个未知近似比的算法：随机撒点，重复对数据点找最近的关键点，关键点 Recenter 到离其最近的数据点的 means。</li>
</ul>
<p>关键点的选择对种子非常敏感，有些收敛很慢或者收敛到非最优解。选择是应当尽量让初始种子互相远离并且多次随机。</p>
<h1><span id="神经网络">神经网络</span></h1><ul>
<li><strong>Bias</strong>：改变输出的范围。</li>
<li><strong>激活函数</strong>：Sigmoid，Tanh（多用于限制范围回归），ReLU，Leaky ReLU，Softmax。为神经网络模型提供非线性性。</li>
<li><strong>损失函数</strong>：交叉熵，$L_p$ 范数，MSE（均方误差，即 $L_2$），MAE（平均绝对误差，即 $L_1$）</li>
</ul>
<p>为了做梯度下降，需要求解每一层的梯度。</p>
<p>假设 $f(\boldsymbol{x})$ 为激活函数，$\mathcal{L}$ 为损失函数。层数为 $L$。网络结构为 </p>
<ul>
<li><strong>输出</strong> $\mathcal{L} &#x3D; (\boldsymbol{y} - \boldsymbol{a}^L)^2$</li>
<li><strong>全连接</strong> $\boldsymbol{z}^i &#x3D; \mathbf{W}^{lT}\boldsymbol{a}^{l - 1} + \boldsymbol{b}^l$</li>
<li><strong>激活</strong> $\boldsymbol{a}^l &#x3D; f(\boldsymbol{z}^l)$</li>
</ul>
<p>为了递推我们记录中间结果，令</p>
<p>$$<br>\delta^l &#x3D; \frac{\partial \mathcal{L}}{\partial \boldsymbol{z}^l}<br>$$</p>
<p>那么对于输出层</p>
<p>$$<br>\delta^L &#x3D; \frac{\partial \mathcal{L}}{\partial \boldsymbol{a}^L} f’(\boldsymbol{z}^L)<br>$$</p>
<p>对于其他层</p>
<p>$$<br>\begin{align}<br>\delta^l &amp;&#x3D; \frac{\partial \mathcal{L}}{\partial \boldsymbol{z}^l} \\<br>&amp;&#x3D; \frac{\partial \mathcal{L}}{\partial \boldsymbol{z}^{l + 1}}\frac{\partial \boldsymbol{z}^{l + 1}}{\partial \boldsymbol{z}^l} \\<br>&amp;&#x3D; \delta^{l + 1}\frac{\partial}{\partial \boldsymbol{a}^l}((\mathbf{W}^{l+1})^T\boldsymbol{a}^l + \boldsymbol{b}^{l + 1})\frac{\partial \boldsymbol{a}^l}{\partial\boldsymbol{z}^l} \\<br>&amp;&#x3D; \delta^{l + 1} (\mathbf{W}^{l+1})^T f’(\boldsymbol{z}^l)<br>\end{align}<br>$$</p>
<p>通过辅助变量可以直接算出梯度</p>
<p>$$<br>\begin{align}<br>  \frac{\partial \mathcal{L}}{\partial \mathbf{W}^l} &amp;&#x3D; \frac{\partial \mathcal{L}}{\partial \boldsymbol{z}^l}\frac{\partial \boldsymbol{z}^l}{\partial \mathbf{W}^l} &amp;&amp; &#x3D; \delta^l (\boldsymbol{a}^{l-1})^{T} \\<br>  \frac{\partial \mathcal{L}}{\partial \boldsymbol{b}^l} &amp;&#x3D; \frac{\partial \mathcal{L}}{\partial \boldsymbol{z}^l}\frac{\partial \boldsymbol{z}^l}{\partial \boldsymbol{b}^l}  &amp;&amp; &#x3D; \delta^l<br>\end{align}<br>$$</p>
<blockquote>
<p><strong>Remark.</strong> 这个推导看起来很神秘。事实上你如果严格把每一层都写开，它确实就是对的。这里我们简单解说一下上面和下面都是矩阵的求偏导，来让他看起来有道理一点。</p>
<p>$\dfrac{\partial \mathbf{A}}{\partial \mathbf{B}}$ 表示 $\dfrac{\partial}{\partial \mathbf{B}} \otimes \mathbf{A}$，这里 $\otimes$ 表示 Kronecker 积。其中 $\dfrac{\partial}{\partial \mathbf{B}}$ 是一个算子，它作用在一个元 $f$ 上得到的结果和 $\mathbf{}{B}^T$ 有相同的形状，其中</p>
<p>$$<br>  \frac{\partial f}{\partial \mathbf{B}}_{ij} &#x3D; \frac{\partial f}{\partial B_{ji}}<br>$$</p>
<p>你可以自行验证这个算子的线性性，和写成这种形式的求导线性性、链式法则均成立。</p>
</blockquote>
<p><strong>ReLU 函数</strong> 可以用来防止梯度消失，同时可以看到 ReLU 函数激活相当于将某一个点的斜率加上一个值（Slope Trick），因此可以用若干个并排的 ReLU 神经元来拟合复杂曲线。</p>
<p><img src="/../picture/ReLUfitCurve.png"></p>
<ul>
<li><strong>优化方法</strong><ul>
<li>梯度下降</li>
<li>随机梯度下降（SGD）</li>
<li>自适应学习率的梯度下降（RMSProp, Adagrad, Adam, AMSGrad, AdaBound 等）</li>
</ul>
</li>
<li><strong>超参数调整</strong><ul>
<li>在 Validation Set 上调整超参数</li>
</ul>
</li>
<li><strong>防止过拟合</strong><ul>
<li>正则化</li>
<li>数据增强</li>
<li>提前停止</li>
<li>Dropout</li>
</ul>
</li>
</ul>
<h1><span id="cnn">CNN</span></h1><h2><span id="批话">批话</span></h2><p><em>人眼识别有平移不变性，相当于空间上的权重共享</em>。</p>
<ul>
<li><p><strong>卷积核（滤波器）形状</strong>包括</p>
<ul>
<li><strong>Padding</strong>（边框）</li>
<li><strong>Strides</strong>（步长）</li>
<li><strong>Filter size</strong>（矩阵大小）</li>
</ul>
<p>容易根据两块之间的大小差距计算上面三个东西、参数量、感受野大小。</p>
</li>
<li><p><strong>空洞卷积</strong> 可以获得更大的感受野</p>
</li>
<li><p><strong>池化</strong> MaxPooling，MeanPooling，空间金字塔池化。</p>
<p><img src="/../picture/SpatialPyramid.png"></p>
</li>
<li><p><strong>分层表示学习</strong> 堆叠多个层次的卷积层来构建神经网络，目的是获得更大的感受野。越靠近输入的层看到的特征越细。</p>
</li>
<li><p><strong>反卷积</strong> 实现 decode 的过程。</p>
</li>
</ul>
<h2><span id="代表性网络">代表性网络</span></h2><ul>
<li><strong>AlexNet</strong> 创造历史，取得 $10%$ 的性能提升。</li>
<li><strong>VGG16</strong> 连续的 $3\times 3$ 卷积层等效于一个具有更大感受野的卷积层，参数量更小。可以作为 Feature Extractor。</li>
<li><strong>ResNet</strong> 加入残差连接（浅层特征和深层特征求和）。</li>
<li><strong>三个快速网络</strong><ul>
<li><p><strong>Squeeze Net</strong>，其中有一种 Fire Block 机制（将正常卷积和一个 $1\times 1$ 卷积块拼在一起），在 CPU 上非常快。</p>
</li>
<li><p><strong>Mobile Net</strong>，采用深度可分离卷积的 Trick，先对每一个 input channel 做卷积，然后通过逐点卷积对得到的特征进行混合。观察下图可以发现参数量明显减少（$M$：input channels，$N$：output channels，$D_k$：特征图尺寸）。</p>
<p><img src="/../picture/MobileNet.png"></p>
</li>
<li><p><strong>Shuffle Net</strong>，组卷积，同组共享权重，然后打乱层，将不同组的信息合并。</p>
</li>
</ul>
</li>
</ul>
<h2><span id="应用">应用</span></h2><h3><span id="object-detection">Object Detection</span></h3><p>一种监督学习任务。</p>
<p>采用 Jacard Similarity（课件中叫做交并比，Intersection over Union）来判定是否检测成果。如果 Jacard Similarity 大于某个阈值，则认为检测成功。</p>
<p>评估模型一般考虑下面几个比例</p>
<table>
<thead>
<tr>
<th align="center">真实值 \ 预测值</th>
<th align="center">反例</th>
<th align="center">正例</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>反例</strong></td>
<td align="center">TN（真阴性）</td>
<td align="center">FP（假阳性）</td>
</tr>
<tr>
<td align="center"><strong>正例</strong></td>
<td align="center">FN（假阴性）</td>
<td align="center">TP（真阳性）</td>
</tr>
</tbody></table>
<p>一般检查下面两个量</p>
<ul>
<li><strong>精度</strong> $\mathrm{Precision} &#x3D; \dfrac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}}$。</li>
<li><strong>召回</strong> $\mathrm{Recall} &#x3D; \dfrac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}}$。</li>
</ul>
<p>下面是代表性算法。</p>
<ul>
<li><p><strong>R-CNN</strong></p>
<ol>
<li>选择性搜索算法，提取约 2000 个候选区域。我不知道选择性搜索算法是如何工作的。</li>
<li>Resize 每个候选区域，喂给 VGG 生成特征图。</li>
<li>喂给 SVM Classifier。</li>
<li>非极大值抑制。</li>
</ol>
<p>有以下局限：</p>
<ul>
<li>选择性搜索速度慢。</li>
<li>Resize 候选区域影响分类准确性。</li>
<li>每个区域单独输入 VGG 非常慢。</li>
<li>非端到端。</li>
</ul>
</li>
<li><p><strong>SPP Net</strong></p>
<p>先把整张图喂给 VGG 得到特征图，然后在特征图上用选择性搜索得到区域。</p>
<p>对候选区域使用空间金字塔池化，然后喂给 SVM。</p>
<p>解决了 R-CNN 的第二、第三个局限性。</p>
</li>
<li><p><strong>Fast R-CNN</strong> 将空间金字塔池化换成简化版的感兴趣区域池化（ROI），将 SVM 换成神经网络。</p>
<p>实现了近似端到端训练。</p>
</li>
<li><p><strong>Faster R-CNN</strong> 将选择性搜索换成区域提议网络（Region Proposal Network）。</p>
<p>速度更快，实现了真正的端到端训练。</p>
</li>
<li><p><strong>YOLO</strong> 删掉区域提议。首先将图划分成 $7\times 7$ 的网格，每个网格负责预测两个中心在其内部的 Bounding Box 和分类标签，以及匹配到每种物体的概率（注意可以有两个边界框，但只匹配一种物体），最后保留置信度高的边界框，并做非极大值抑制。</p>
<p>只需要看一次图，但其局限性为难以检测小物体。</p>
</li>
<li><p><strong>YOLO v2</strong> 预定义先验框。</p>
</li>
<li><p><strong>Single Shot Multibox Detector（SSD）</strong> 另一种只看一次的算法。</p>
</li>
</ul>
<h3><span id="image-segmentation">Image Segmentation</span></h3><p>细粒度的分类。使用全卷积神经网络。</p>
<p><img src="/../picture/FCN.png"></p>
<p>随着层数增加，低级别特征可能会丢失，因此考虑跳跃连接。</p>
<p><img src="/../picture/SkipConnection.jpg"></p>
<p>用了这个 trick 的两个网络：</p>
<ul>
<li><p><strong>SegNet</strong></p>
<p><img src="/../picture/SegNet.png"></p>
</li>
<li><p><strong>PSPNet</strong>（Pyramid Scene Parsing Network）</p>
<p><img src="/../picture/PSPNet.png"></p>
</li>
</ul>
<p>衡量模型表现可以考虑：</p>
<ul>
<li><p><strong>Pixel-wise cross entropy</strong>，对每个像素算交叉熵，对于分割小物体的性能较差。</p>
</li>
<li><p><strong>Dice coefficient</strong>，将预测区域看成 01 向量（按是否在内部）定义</p>
<p>$$<br>\mathrm{Dice} &#x3D; \frac{2|A\cdot B|}{|A| + |B|}<br>$$</p>
<p>类似于 IoU，但是可以求导。</p>
</li>
</ul>
<p>图像分割的一个更强的任务是 Instance Segmentation。</p>
<p>$$<br>\mathrm{Instance Segmentation} &#x3D; \mathrm{Object Classification} + \mathrm{Object Detection} + \mathrm{Semantic Segmentation}<br>$$</p>
<p>代表算法是 <strong>Mask R-CNN</strong>。</p>
<p>Segmentation 的时候容易丢失小物体和边界信息，因此有如下技巧：</p>
<ul>
<li><strong>镜像填充</strong>。</li>
<li><strong>损失加权</strong>，增加边缘权重、小物体权重。</li>
</ul>
<h3><span id="face-recognition">Face Recognition</span></h3><ol>
<li>检测图像中的人脸位置。</li>
<li>将人脸对齐到图像中心。</li>
<li>身份识别。</li>
</ol>
<p>可以进行分类</p>
<ul>
<li><strong>Identification</strong>（识别出来是谁）</li>
<li><strong>Verification</strong>（判断是不是给定人）</li>
</ul>
<p>按照是否动态分为</p>
<ul>
<li><strong>Close-set</strong>，人脸数据不会变化，简单的分类问题。</li>
<li><strong>Open-set</strong>，会输入新的数据，注意你不能重新训练模型。</li>
</ul>
<p>Open-set 的问题是特征提取问题。本质上是通过监督学习来学习区分特征空间。要点是设计新的网络架构和损失函数。</p>
<hr>
<p>其他一些应用：</p>
<ul>
<li><strong>Pose Estimation</strong><ul>
<li><strong>Convolutional Pose Machine, CPM</strong> Top-down 算法。先找人的边界框，然后提取特征，再用 CNN 输出关键点的 heatmap。</li>
<li><strong>OpenPose</strong>，CPM + Bottom-up。在 CPM 的基础上输出肢节的 heatmap。</li>
<li><strong>Pose Proposal Networks, PPN</strong>，YOLO + OpenPose（视为目标检测问题）。</li>
</ul>
</li>
</ul>
<h1><span id="gan">GAN</span></h1><p><em>判别式模型寻找<strong>决策边界</strong>，生成式模型寻找<strong>联合分布</strong>。</em></p>
<p><img src="/../picture/GAN.png"></p>
<p>其中 $G$ 相当于构造一个从 Gauss 向量到另一个向量的一一映射。$G$ 的目标是生成像是数据集中的图像，$D$ 的目标是区分图像是否来自数据集。</p>
<p>分开来看，两个网络的损失函数分别是 </p>
<p>$$<br>\begin{align}<br>  \mathcal{L}_D &amp;&#x3D; -\mathbb{E}_{\boldsymbol{x} \sim p{data}} \log D(\boldsymbol{x}) - \mathbb{E}_{\boldsymbol{z}\sim p_z}\log(1 - D(G(\boldsymbol{z}))) \\<br>  \mathcal{L}_G &amp;&#x3D; -\mathbb{E}_{\boldsymbol{z}\sim p_z}\log D(G(\boldsymbol{z}))<br>\end{align}<br>$$</p>
<p>感性理解，我们可以直接优化</p>
<p>$$<br>\min_G\max_D V(D, G) &#x3D; \mathbb{E}_{\boldsymbol{x} \sim p_{data}} \log D(\boldsymbol{x}) + \mathbb{E}_{\boldsymbol{z}\sim p_z}\log(1 - D(G(\boldsymbol{z})))<br>$$</p>
<p>注意后一项的增减性。定义 $p_g$ 为 $\boldsymbol{z}\sim p_z$ 时 $G(\boldsymbol{z})$ 的概率分布。那么损失函数变成</p>
<p>$$<br>\min_G\max_D V(D, G) &#x3D; \mathbb{E}_{\boldsymbol{x} \sim p_{data}} \log D(\boldsymbol{x}) + \mathbb{E}_{\boldsymbol{z}\sim p_g}\log(1 - D(\boldsymbol{z}))<br>$$</p>
<div class="note info"><p><strong>Claim.</strong> 该损失函数达到全局最优解时 $p_g &#x3D; p_{data}$。</p>
<p><strong>Proof.</strong></p>
<p>考虑 $G$ 固定，那么 </p>
<p>$$<br>V(G, D) &#x3D; \int_{\boldsymbol{x}} p_{data}\log D(x) + p_g(\boldsymbol{x})\log(1 - D(\boldsymbol{x}))\mathrm{d}x<br>$$</p>
<p>现在 $D$ 想要最大化这个函数，需要放出被积函数的一个下界，根据费马原理达到下界时必然有（注意这里把 $D(\boldsymbol{x})$ 看成变量）</p>
<p>$$<br>\frac{\partial}{\partial D(\boldsymbol{x})}p_{data}\log D(x) + p_g(\boldsymbol{x})\log(1 - D(\boldsymbol{x})) &#x3D; \frac{p_{data(\boldsymbol{x})}}{D(\boldsymbol{x})} - \frac{p_g(\boldsymbol{x})}{1 - D(\boldsymbol{x})}<br>$$</p>
<p>注意此时 $D$ 时全局最优解，因此最优的 $D^*$ 满足</p>
<p>$$<br>D^* &#x3D; \frac{p_{data}}{p_g + p_{data}}<br>$$</p>
<p>接下来固定 $D^*$，$G$ 想要最小化</p>
<p>$$<br>\mathbb{E}_{\boldsymbol{x}\sim p_{data}} \log \frac{p_{data}(\boldsymbol{x})}{p_{data}(\boldsymbol{x}) + p_g(\boldsymbol{x})} + \mathbb{E}_{\boldsymbol{x}\sim p_g}\log \frac{p_g(\boldsymbol{x})}{p_{data}(\boldsymbol{x}) + p_g(\boldsymbol{x})}<br>$$</p>
<p>这个式子和两个 KL 散度的和只差一个常数（分母必须是一个分数），我们通过减去 $2\log 2$ 来进行微调，原式等于</p>
<p>$$<br>-2\log 2 + D_{KL}(p_{data} || \frac{p_d + p_{data}}{2}) + D_{KL}(p_{g} || \frac{p_d + p_{data}}{2})<br>$$</p>
<p>根据 Gibbs 不等式取下界当且仅当 $p_g &#x3D; p_{data} &#x3D; \frac{p_d + p_{data}}{2}$，解得此时 $p_g &#x3D; p_{data}$</p>
</div>

<p>训练时应当分多步反复训练 $G$ 和 $D$ 而不是一开始就将 $D$ 训练至最优：给 $G$ 一个优化的动力。</p>
<h2><span id="经典算法">经典算法</span></h2><ul>
<li><strong>DCGAN</strong> 使用卷积神经网络。使用的 technique 如下<ul>
<li>对所有层批量归一化，除了 $G$ 的最后一层和 $D$ 的输入层，衰减系数 $0.9$。</li>
<li>使用 Adam，一阶动量 $0.5$。</li>
<li>$0.2\alpha$ 的 Leaky ReLU。</li>
<li>跨步卷积替代最大池化。</li>
<li>学习率 $0.0001$。</li>
</ul>
</li>
</ul>
<p>在 DCGAN 之前使用 VAE 做生成式任务（训练一个 Encoder 从 data 打到高斯分布，然后训练一个 Decoder，计算 $L_2(\boldsymbol{x}, D(E(\boldsymbol{x})))$）。VAE 可以学习 Latent Representation（Noise vector 的线性组合具有语义性）</p>
<p>采用 MSE 的 VAE 不能重构小物体，但是用对抗损失和 MSE 结合可以重构小物体，因为小物体遵循可预测的模式。</p>
<ul>
<li><p><strong>Conditional GAN</strong> 多一个分类标签。损失函数上面加上和 $c$ 有关系的信息。</p>
<p>套上 RNN 可以做文生图。</p>
<p><img src="/../picture/conGAN.png"></p>
</li>
</ul>
<hr>
<p>现在希望 GAN 也可以学习 Latent Representation。</p>
<p>采用暴力的方法，直接训练 Encoder</p>
<p><img src="/../picture/BruteLatent.jpg"></p>
<p>效果都不好（前者没有见过真图，G 不能生成所有图片；后者 G 不一定能生成给定的图片）。</p>
<ul>
<li><p><strong>BiGAN</strong> D 的读入是一张图及其 Latent Representation。</p>
<p><img src="/../picture/BiGAN.jpg"></p>
</li>
<li><p><strong>CoGAN</strong> 寻找两个领域的联合分布（希望能做图像翻译）</p>
<p><img src="/../picture/CoGAN.png"></p>
<p>但是这个给定一个领域的图片不能生成另一个领域的图片，还需要训练一个 Encoder 打回 Latent Represectation。</p>
</li>
<li><p><strong>Cycle GAN</strong> 解决上面的问题。</p>
<p><img src="/../picture/CycleGAN.jpg"></p>
<p>希望如果在 $G_B$ 中输入了 $A$ 还是得到 $A$，故引入 Identity loss：</p>
<p>$$<br>L_2(X_A, G_A(E_B(X_A)))<br>$$</p>
</li>
</ul>
<h1><span id="rnn">RNN</span></h1><p>时间序列分析用模型。</p>
<p>首先考虑词的表示。</p>
<ul>
<li><p><strong>Onehot</strong>。维数灾难，过于稀疏。</p>
</li>
<li><p><strong>Bag of Words</strong>。维数灾难，丢失位置信息。</p>
</li>
<li><p><strong>Word Embedding</strong>。要求包含语义信息，相似单词有相似表示，算术运算和语义操作同构。</p>
<p>自监督学习（通过比较上下文来找嵌入模式）。</p>
<ul>
<li><p><strong>Word2Vec</strong>：CBOW&#x2F;Skip-Gram + 负采样</p>
<p>连续词袋模型（CBOW），根据上下文窗口预测中间词。</p>
<p>Skip-Gram 给定中间词，最大化上下文的概率。这里因为输出的是上下文的词出现的概率，所以用 Sigmoid 而不是 Softmax。当词汇表大的时候计算量很大，因此考虑直接采样近似。</p>
</li>
</ul>
</li>
</ul>
<hr>
<p>接下来给出朴素 RNN 的结构。</p>
<p><img src="/../picture/VanillaRNN.png"></p>
<p>其中 $h_t$ 表示当前时间步的某种信息，$y_t$ 表示当前步的输出，$\sigma$ 是某个激活函数。</p>
<p>$$<br>\begin{align}<br>  h_t &amp;&#x3D; \tanh(W_hx_t + U_hh_{t - 1} + b_h) \\<br>  y_t &amp;&#x3D; \sigma(W_yh_t + b_y)<br>\end{align}<br>$$</p>
<p>难以解决长期记忆问题，因此提出 <strong>LSTM</strong>（长短期记忆网络）。</p>
<p>首先定义门函数（Gate Function），是一个诸维度都在 $[0, 1]$ 的向量，用于控制记忆、遗忘某些维度的信息。</p>
<p>额外定义 $c_t$ 表示时间 $t$ 时的 Cell Vector，用来从前往后传递信息。</p>
<p>将 $h_{t-1}$ 和 $x_t$ concat 起来，通过三个线性变换算出三个门（Sigmoid 激活）。LSTM 的循环单元如图</p>
<p><img src="/../picture/LSTM.png"></p>
<ul>
<li>可以用 ReLU 替换 Sigmoid 函数。</li>
</ul>
<hr>
<p>考虑几个经典任务。</p>
<ul>
<li><strong>Many-to-one</strong> 情感分类等。</li>
<li><strong>One-to-many</strong> 图片描述等。每一个时间步的输出作为下一个时间步的输入。用所有输出来计算损失（如平均交叉熵等）。</li>
<li><strong>同步 Many-to-many</strong> 交通计数等。要保证每张图都对，所以说需要预先定义序列长度来计算损失。以及文本生成。</li>
<li><strong>异步 Many-to-many</strong> Chatbot。</li>
</ul>
<h1><span id="transformer">Transformer</span></h1><p><img src="/../picture/Transformer.png"></p>
<h2><span id="网络结构">网络结构</span></h2><ul>
<li><strong>非自主性提示</strong> Key，Value</li>
<li><strong>自主性提示</strong> Query</li>
</ul>
<p><strong>自注意力机制</strong></p>
<p>$$<br>\mathrm{Attention}(Q, K, V) &#x3D; \mathrm{softmax}\left(\frac{K^TQ}{\sqrt {D_k}}\right) V<br>$$</p>
<p>softmax 以及里面包的东西成为注意力权重矩阵，因为 $K^TQ_{ij}$ 实际上是 $\langle K_i, Q_j\rangle$ 所以确实是某种相似度的计算。除掉 $\sqrt {D_k}$ 是为了防止点积过大，梯度过小。</p>
<p><strong>多头注意力</strong>：将多个 self-attention 的结果 concat 起来。</p>
<p><strong>位置编码</strong></p>
<p>$$<br>\begin{align}<br>\mathrm{PE}(pos, 2i) &amp;&#x3D; \sin\left(\frac{pos}{10000^{2i &#x2F; d_{\mathrm{model}}}}\right) \\<br>\mathrm{PE}(pos, 2i + 1) &amp;&#x3D; \cos\left(\frac{pos}{10000^{2i &#x2F; d_{\mathrm{model}}}}\right)<br>\end{align}<br>$$</p>
<p>输入等于词向量 + 位置编码。</p>
<p><strong>残差连接</strong> 将输入线性变换，使得其和注意力输出维度匹配，然后将两者相加，并且做 Layer Normalization。</p>
<p><strong>逐位前馈网络</strong> 两个全连接层，第一个之后用 ReLU 激活。</p>
<p><strong>多编码器堆叠</strong> 将多个编码器串联起来，得到最终输出。</p>
<p><strong>掩蔽多头注意力</strong> 在 $K^TQ$ 上面加上 attention mask 矩阵，该矩阵形如</p>
<p>$$<br>\begin{pmatrix}<br>  0 &amp; -\infty &amp; \cdots &amp; -\infty &amp; -\infty \\<br>  0 &amp; 0 &amp; \cdots &amp; -\infty &amp; -\infty  \\<br>  \vdots &amp; \vdots &amp;  &amp; \vdots &amp; \vdots \\<br>  0 &amp; 0 &amp; \cdots &amp; 0 &amp; -\infty \\<br>  0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0  \\<br>\end{pmatrix}<br>$$</p>
<p>该矩阵屏蔽了后面的信息（生成文本时不需要考虑下文）。</p>
<p>掩蔽多头注意力的输出作为 $Q$，和编码器输出作为 $K, V$ 再过一个多头注意力。</p>
<h2><span id="预训练模型">预训练模型</span></h2><p><em>在无标签数据上学习通用语言表示，从而提高 NLP 任务的性能</em>。</p>
<ol>
<li><strong>预训练阶段</strong> 再大量无标签文本数据上预训练。</li>
<li><strong>微调阶段</strong> 针对特定任务进行微调。</li>
</ol>
<ul>
<li><p><strong>GPT</strong> 学习单向上下文信息。</p>
<p>使用 Transformer 的 Decoder 堆叠。无监督学习，最大化对下一个词的预测概率。</p>
<p>微调阶段D为特定任务添加相关的头部（分类层、标注层）。</p>
<p><img src="/../picture/GPTRefine.jpg"></p>
</li>
<li><p><strong>BERT</strong> 学习双向上下文信息。</p>
<p>使用 Transformer 的 Encoder 堆叠。有两个任务：</p>
<ul>
<li><strong>MLM</strong>（Masked Language Modeling） 将一些词替换为 <code>&lt;SEP&gt;</code>，做完形填空。</li>
<li><strong>NSP</strong>（Next Sentence Prediction）输入两个连续的句子，判断是否为连续上下文。用 Segment Embedding 来使模型可以区分多个句子。</li>
</ul>
</li>
</ul>
<p>大模型参数规模提升带来了能力<strong>涌现</strong>。（模型基本结构和训练方式基本不变，只增大模型和数据规模）原因为</p>
<ul>
<li><strong>大量的训练数据</strong>：人类积累的所有信息</li>
<li><strong>模型容量</strong>：充分学习数据</li>
<li><strong>自回归和无监督训练</strong>：不需要标注海量数据</li>
<li><strong>迁移学习和微调</strong>：适应不同任务</li>
<li><strong>多任务学习</strong>：提高泛化性</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/03/04/TensorAnal3TensorCalc/" rel="prev" title="Tensor Analysis (3) Tensor Calculus">
                  <i class="fa fa-angle-left"></i> Tensor Analysis (3) Tensor Calculus
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/06/04/AdvMathRevision2/" rel="next" title="Revision | 高等数学 A II（期末）">
                  Revision | 高等数学 A II（期末） <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">King Strange</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
