<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.0.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":"flat","style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"utterances","storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="策略梯度的推导 算法的问题 高方差 Baseline Trick Reward-to-go Trick 折扣因子   采样效率低      本部分介绍策略梯度相关的方法。这是一种 Model-Free 的强化学习算法，即假设不能知道转移概率。 策略梯度的推导回忆在强化学习中我们需要优化的目标是 $${J}(\theta) &#x3D; \mathbb{E}_{\tau\sim p_\thet">
<meta property="og:type" content="article">
<meta property="og:title" content="CS285 Part 3 - 策略梯度方法">
<meta property="og:url" content="http://example.com/2025/02/08/CS285DRL5/index.html">
<meta property="og:site_name" content="Stellary&#39;s Notes">
<meta property="og:description" content="策略梯度的推导 算法的问题 高方差 Baseline Trick Reward-to-go Trick 折扣因子   采样效率低      本部分介绍策略梯度相关的方法。这是一种 Model-Free 的强化学习算法，即假设不能知道转移概率。 策略梯度的推导回忆在强化学习中我们需要优化的目标是 $${J}(\theta) &#x3D; \mathbb{E}_{\tau\sim p_\thet">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-02-08T05:15:17.000Z">
<meta property="article:modified_time" content="2025-02-09T11:52:50.509Z">
<meta property="article:author" content="King Strange">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/2025/02/08/CS285DRL5/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2025/02/08/CS285DRL5/","path":"2025/02/08/CS285DRL5/","title":"CS285 Part 3 - 策略梯度方法"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>CS285 Part 3 - 策略梯度方法 | Stellary's Notes</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Stellary's Notes</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">策略梯度的推导</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">2.</span> <span class="nav-text">算法的问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">2.1.</span> <span class="nav-text">高方差</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.1.1.</span> <span class="nav-text">Baseline Trick</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.1.2.</span> <span class="nav-text">Reward-to-go Trick</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.1.3.</span> <span class="nav-text">折扣因子</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">2.2.</span> <span class="nav-text">采样效率低</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="King Strange"
      src="https://s2.loli.net/2023/11/24/rtCEFG1igyYAlm4.png">
  <p class="site-author-name" itemprop="name">King Strange</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">59</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">42</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/SocialZxy" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;SocialZxy" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:stellary@stu.pku.edu.cn" title="E-Mail → mailto:stellary@stu.pku.edu.cn" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          Links
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://hexo.io/" title="https:&#x2F;&#x2F;hexo.io" rel="noopener" target="_blank">Hexo</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://theme-next.js.org/" title="https:&#x2F;&#x2F;theme-next.js.org&#x2F;" rel="noopener" target="_blank">NexT</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://syksykccc.github.io/" title="https:&#x2F;&#x2F;syksykccc.github.io" rel="noopener" target="_blank">syksykCCC</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://blog.imyangty.com/" title="https:&#x2F;&#x2F;blog.imyangty.com" rel="noopener" target="_blank">YangTY's Blog - 越过山川</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/08/CS285DRL5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://s2.loli.net/2023/11/24/rtCEFG1igyYAlm4.png">
      <meta itemprop="name" content="King Strange">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Stellary's Notes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="CS285 Part 3 - 策略梯度方法 | Stellary's Notes">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          CS285 Part 3 - 策略梯度方法
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-02-08 13:15:17" itemprop="dateCreated datePublished" datetime="2025-02-08T13:15:17+08:00">2025-02-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-02-09 19:52:50" itemprop="dateModified" datetime="2025-02-09T19:52:50+08:00">2025-02-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Notes/" itemprop="url" rel="index"><span itemprop="name">Notes</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><!-- toc -->

<ul>
<li><a href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%8E%A8%E5%AF%BC">策略梯度的推导</a></li>
<li><a href="#%E7%AE%97%E6%B3%95%E7%9A%84%E9%97%AE%E9%A2%98">算法的问题</a><ul>
<li><a href="#%E9%AB%98%E6%96%B9%E5%B7%AE">高方差</a><ul>
<li><a href="#baseline-trick">Baseline Trick</a></li>
<li><a href="#reward-to-go-trick">Reward-to-go Trick</a></li>
<li><a href="#%E6%8A%98%E6%89%A3%E5%9B%A0%E5%AD%90">折扣因子</a></li>
</ul>
</li>
<li><a href="#%E9%87%87%E6%A0%B7%E6%95%88%E7%8E%87%E4%BD%8E">采样效率低</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<p>本部分介绍策略梯度相关的方法。这是一种 Model-Free 的强化学习算法，即假设不能知道转移概率。</p>
<h1><span id="策略梯度的推导">策略梯度的推导</span></h1><p>回忆在强化学习中我们需要优化的目标是</p>
<p>$$<br>{J}(\theta) &#x3D; \mathbb{E}_{\tau\sim p_\theta(\tau)}\left[\sum_{t}r(s_t, a_t)\right]<br>$$</p>
<p>其中 $\tau &#x3D; \{(s_1, a_1, …, s_T, a_T)\}$ 是一个 trajectory，分布 $p$ 在上一节中有给出：</p>
<p>$$<br>p_{\theta}(\mathbf{s}_1, \mathbf{a}_1, …, \mathbf{s}_T, \mathbf{a}_T) &#x3D; p(\mathbf{s}_1)\prod_{t &#x3D; 1}^T \pi_{\theta}(\mathbf{a}_t | \mathbf{s}_t)p(\mathbf{s}_{t + 1} | \mathbf{s}_t, \mathbf{a}_t)<br>$$</p>
<p>我们尝试推导 $\nabla_\theta J(\theta)$。</p>
<p>$$<br>\begin{aligned}<br>\nabla_\theta J(\theta) &amp;&#x3D; \nabla_\theta\int p_{\theta}(\tau) r(\tau) \mathrm{d}\tau \\<br>&amp;&#x3D; \int p_\theta(\tau)\nabla_\theta \log p_\theta(\tau) r(\tau) \mathrm{d}\tau<br>\\<br>&amp;&#x3D; \mathbb{E}_{\tau\sim p(\tau)}\left[\sum_t \nabla_\theta \log \pi_\theta(a_{t} | s_{t})\sum_t r(a_{t}, s_{t})\right]<br>\end{aligned}<br>$$</p>
<p>注意 $\log p(s_{t + 1} | s_t, a_t)$ 和 $\theta$ 无关，所以求偏导之后没有这一项。此时我们可以用采样一个大小为 $N$ 的 batch 来估计这个期望：</p>
<p>$$<br>\boxed{\nabla_\theta J(\theta)\approx \frac{1}{N}\sum_{i&#x3D;1}^N\sum_t \nabla_\theta \log \pi_\theta(a_{i, t} | s_{i, t})\sum_t r(a_{i, i}, s_{i, t})} \label{directpolicygrad}<br>$$</p>
<p>这就是策略梯度的计算方法。在本节末尾我们略微在机器学习的视角下谈一些这个式子的意义。</p>
<p>一方面，从直觉上来讲，奖励更大的策略将产生更大的梯度，参数确实将朝着奖励大的决策方向下降。</p>
<p>回忆在 imitation learning 中，我们本质上是在对专家策略做极大似然估计，专家策略的 likelihood 的对数就是</p>
<p>$$<br>\frac{1}{N}\sum_{i&#x3D;1}^N\sum_{t} \log \pi_\theta(a_{i, t} | s_{i, t})<br>$$</p>
<p>而上面的式子积分之后得到的 pseudo-loss（不是真正优化的东西，但是其梯度正好是原来函数的梯度）是</p>
<p>$$<br>\frac{1}{N}\sum_{i&#x3D;1}^N\sum_t \log \pi_\theta(a_{i, t} | s_{i, t})\sum_t r(a_{i, t}, s_{i, t})<br>$$</p>
<p>虽然这里策略是模型的策略而非专家策略，但是后面乘上了一个关于奖励的系数 $r$，这等价于在采样时，奖励更大的轨迹有更大概率被采样。于是策略梯度算法实际上是在最大化高奖励轨迹的似然。</p>
<p>此外，如果我们只有 observation 而没有完整的 state，需要注意的是得到观察 $o_t$ 的概率仅仅是在得到 $s_t$ 的概率上面乘一个与 $\theta$ 无关的条件概率，所以只知道 observation 时需要优化的式子是</p>
<p>$$<br>\begin{aligned}<br>\nabla_\theta J(\theta)&amp;\approx \frac{1}{N}\sum_{i&#x3D;1}^N\sum_t \nabla_\theta \log \pi_\theta(a_{i, t} | o_{i, t}) r(\tau)<br>\end{aligned}<br>$$</p>
<p>注意最后这个求和在 policy gradient 当中只是一条轨迹的 reward（并已经在模拟后给出），所以它是怎么算出来的毫不重要（到底是 $r(s, a)$ 还是 $r(o, a)$）。因此即便只知道 observation，我们的梯度仍然是正确的。</p>
<h1><span id="算法的问题">算法的问题</span></h1><p>在实际操作中，我们发现用梯度下降方法去做 RL 会很困难，这里列举一些具体原因。</p>
<h2><span id="高方差">高方差</span></h2><blockquote>
<p><strong>Remark.</strong> 这部分网上很多东西很没有道理，课程中给出的例子看起来也很没有道理，我们直接按着自己的思路来写。</p>
</blockquote>
<p>注意在 $\ref{directpolicygrad}$ 中我们用采样的方法来估计梯度，这样采样自然得到的是一个无偏估计。然而在 MDP 中采样，方差几乎是随时间累计的，这就是策略梯度的高方差问题，它导致估计的准确性是很难 bound 住的。我们期望找一些方法来在不增加采样量的前提下降低采样的方差。</p>
<h3><span id="baseline-trick">Baseline Trick</span></h3><div class="note info"><p><strong>引理 1.</strong> 给奖励函数加上一个常数 $b$ 将不会改变梯度的期望。即 </p>
<p>$$<br>\nabla_\theta \mathbb{E}_{\tau \sim p_\theta(\tau)}\left[\sum_{t} r(s_t, a_t) - b\right] &#x3D; \nabla_\theta \mathbb{E}_{\tau \sim p_\theta(\tau)}\left[\sum_{t} r(s_t, a_t)\right]<br>$$</p>
</div>

<p>但是此举会影响梯度的方差，简便起见记 $\nabla_\theta \log \pi_\theta(a_{t} | s_{t}) &#x3D; g(\tau)$：</p>
<p>$$<br>\begin{aligned}<br>&amp;\mathrm{Var}\left[g(\tau)^2(r(\tau) - b)\right] \\<br>&#x3D;&amp; \mathbb{E}[g(\tau)^2(r(\tau) - b)^2] - \mathbb{E}[\text{Grad}]^2 \\<br>&#x3D;&amp; \mathrm{Var}[\text{Grad}] - 2b\mathbb{E}[g(\tau)^2r(\tau)] + b^2\mathbb{E}[g(\tau)^2]<br>\end{aligned}<br>$$</p>
<p>取</p>
<p>$$<br>b &#x3D; \frac{\mathbb{E}[g(\tau)^2r(\tau)]}{\mathbb{E}[g(\tau)^2]}<br>$$</p>
<p>立即得到最优的方差。这实际上就是奖励的一个加权平均作为 baseline，策略向优于 baseline 的方向移动。当然直接取</p>
<p>$$<br>b &#x3D; \frac 1N \sum_{i &#x3D; 1}^n r(\tau)<br>$$</p>
<p>也可以跑的非常好。</p>
<h3><span id="reward-to-go-trick">Reward-to-go Trick</span></h3><p>重新审视 $\ref{directpolicygrad}$，发现这里我们相当于给每一步操作都乘上了它所在的 trajectory 的全部奖励，直觉上来说我们只需要乘这一步以后的奖励。不妨做如下假设：</p>
<div class="note success"><p><strong>假设（Causality Assumption）.</strong> 对于 $t &lt; t’$，$t’$ 时刻的操作和 $t$ 时刻的奖励、操作都独立。换言之，我们的 Agent 只需要考虑行动的后果。</p>
</div>

<p>我们对梯度的式子进行一些变形：</p>
<p>$$<br>\begin{aligned}<br>\nabla_{\theta}J(\theta) &amp;&#x3D;  \mathbb{E}_{\tau\sim p(\tau)}\left[\sum_t \nabla_\theta \log \pi_\theta(a_{t} | s_{t})\sum_t r(a_{t}, s_{t})\right] \\<br>&amp;&#x3D; \mathbb{E}_{\tau\sim p(\tau)}\left[\sum_t \nabla_\theta \log \pi_\theta(a_{t} | s_{t})\sum_{t’ \geq t} r(a_{t’}, s_{t’})\right] \\<br>&amp;+ \color{orange}\mathbb{E}_{\tau\sim p(\tau)}\left[\sum_t \nabla_\theta \log \pi_\theta(a_{t} | s_{t})\sum_{t’ &lt; t} r(a_{t’}, s_{t’})\right]<br>\end{aligned}<br>$$</p>
<p>现在来分析橙色部分具体是什么。</p>
<p>$$<br>\begin{aligned}<br>    &amp;\mathbb{E}_{\tau\sim p(\tau)}\left[\sum_{t’} r(a_{t’}, s_{t’})\sum_{t &gt; t’} \nabla_\theta \log \pi_\theta(a_{t} | s_{t})\right] \\<br>    &#x3D;~&amp; \sum_{t’}\sum_{t &gt; t’} \mathbb{E}[r(a_{t’}, s_{t’})\nabla_\theta\log\pi_\theta(a_t | s_t)] \\<br>    &#x3D;~&amp; \sum_{t’} \mathbb{E}[r(a_{t’}, s_{t’})]\sum_{t &gt; t’} \color{blue}\mathbb{E}[\nabla_\theta\log\pi_\theta(a_t | s_t)]<br>\end{aligned}<br>$$</p>
<p>末尾蓝色的部分非常眼熟，它正是</p>
<p>$$<br>1 &#x3D; \int p(\tau) \mathrm{d}\tau<br>$$</p>
<p>的梯度，注意 $\pi_\theta(a_t | s_t)$ 无非是 $p(\tau)$ 的一个边缘分布。</p>
<p>于是橙色的部分是 $0$，计算梯度只需要未来的奖励：</p>
<p>$$<br>\nabla_{\theta}J(\theta) &#x3D; \mathbb{E}_{\tau\sim p(\tau)}\left[\sum_t \nabla_\theta \log \pi_\theta(a_{t} | s_{t})\sum_{t’ \geq t} r(a_{t’}, s_{t’})\right]<br>$$</p>
<p>直觉上，这种方法通过缩小了变量的值域来减小了方差。因此也被归类在解决“高方差”这一小节之下。实际上这个方法的可能更加利好后面的 actor-critic 模型的推导。</p>
<p>如果用采样来计算，需要优化的式子即为</p>
<p>$$<br>\boxed{\frac 1N \sum_{i&#x3D;1}^n\sum_t \nabla_\theta \log \pi_\theta(a_{i, t} | s_{i, t})\sum_{t’ \geq t} r(a_{i, t’}, s_{i, t’})} \label{reward-to-go-policy-grad}<br>$$</p>
<h3><span id="折扣因子">折扣因子</span></h3><p>通过让每一时间步获得的 Reward 都乘上 $\gamma$ 来减小方差。下面的式子用来折扣普通的策略梯度：</p>
<p>$$<br>\nabla_\theta J(\theta)\approx \frac{1}{N}\sum_{i&#x3D;1}^N\sum_t \nabla_\theta \log \pi_\theta(a_{i, t} | s_{i, t})\sum_t {\color{red}{\gamma^{t - 1}}}r(a_{i, i}, s_{i, t})<br>$$</p>
<p>而带上 causality assumption 的版本则是</p>
<p>$$<br>\frac 1N \sum_{i&#x3D;1}^n\sum_t \nabla_\theta \log \pi_\theta(a_{i, t} | s_{i, t})\sum_{t’ \geq t} {\color{red}{\gamma^{t’ - t}}}r(a_{i, t’}, s_{i, t’})<br>$$</p>
<h2><span id="采样效率低">采样效率低</span></h2><p>注意我们要优化的式子中，优化一轮就需要采样一轮，开销非常大。现在考虑是否有办法在原来的分布上进行采样。</p>
<div class="note info"><p><strong>引理 2（Importance Sampling）.</strong> </p>
<p>$$<br>\mathbb{E}_{x\sim p(x)}[f(x)] &#x3D; \mathbb{E}_{x\sim q(x)} \left[\frac{p(x)}{q(x)}f(x)\right]<br>$$</p>
</div>

<p>假设我们此前从一个旧版本的神经网络 $\bar\pi$ 中采样了一组 trajectory，那么我们需要计算</p>
<p>$$<br>\frac{\pi_\theta(\tau)}{\bar{\pi}(\tau)} &#x3D; \frac{p(s_1)\prod p(s_{t + 1} | s_t, a_t){\pi}_\theta(a_t | s_t)}{p(s_1) \prod p(s_{t + 1} | s_t, a_t)\bar{\pi}(a_t | s_t)} &#x3D; \frac{\prod {\pi}_\theta(a_t | s_t)}{\prod \bar{\pi}(a_t | s_t)}<br>$$</p>
<p>套上 Importance Sampling 得到优化目标函数</p>
<p>$$<br>J(\theta) &#x3D; \mathbb{E}_{\tau \sim \bar\pi(\tau)}\left[\frac{\pi_\theta(\tau)}{\bar\pi(\tau)}r(\tau)\right]<br>$$</p>
<p>所有和 Model 相关的东西都消掉了，因此此方法是可行的。那么简单求导知道：</p>
<p>$$<br>\nabla_\theta J(\theta) &#x3D; \mathbb{E}_{\tau \sim \bar\pi(\tau)}\left[\prod_t \frac{\pi_{\theta}(a_t | s_t)}{\bar\pi(a_t | s_t)}\sum_t \nabla_\theta \log \pi_\theta(a_{t} | s_{t})\sum_t r(a_{t}, s_{t})\right]<br>$$</p>
<p>如果带上 causality assumption，有</p>
<p>$$<br>\nabla_\theta J(\theta) &#x3D; \mathbb{E}_{\tau \sim \bar\pi(\tau)}\left[\sum_t \nabla_\theta \log \pi_\theta(a_{t} | s_{t})\prod_{t’ \leq t} \frac{\pi_{\theta}(a_{t’} | s_{t’})}{\bar\pi(a_{t’} | s_{t’})}\sum_{t’’ \geq t} r(a_{t’’}, s_{t’’})\right]<br>$$</p>
<blockquote>
<p><strong>Remark.</strong> 这个就有点不知道是什么道理了。</p>
</blockquote>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/01/14/CS285DRL4/" rel="prev" title="CS285 Part 2 - RL 概述">
                  <i class="fa fa-angle-left"></i> CS285 Part 2 - RL 概述
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/03/05/AIPaperReading/" rel="next" title="泛 AI 论文选读">
                  泛 AI 论文选读 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">King Strange</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
